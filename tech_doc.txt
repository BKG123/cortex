Cortex — Technical Specification (implementation-ready)
Purpose: a compact, opinionated, production-ready memory library for LLM apps.
 Goal: one simple input API (.ingest(...)) and a powerful, token-efficient .get_context(query) plus structured retrieval outputs. Defaults are local-first (SQLite + FAISS-like), but with clear abstraction points for PG/pgvector, OpenAI embeddings, and hosted modes.

1. High-level architecture (single-file overview)
Client App
   └─> cortex.Cortex                      # public API
         ├─> Ingest Pipeline
         │     ├─> Preprocessor (normalization)
         │     ├─> Classifier (rules + optional LLM)
         │     ├─> Extractor (entity/date/preferences)
         │     └─> Router -> Memory Stores
         ├─> Memory Stores (opinionated defaults)
         │     ├─> EpisodicStore  (SQL table)
         │     ├─> PreferenceStore (SQL / JSON)
         │     ├─> LongTermStore (SQL + optional graph)
         │     └─> SemanticStore (FAISS / PGVector)
         └─> Retrieval Engine
               ├─> Query planner (assemble subqueries)
               ├─> Ranker (hybrid scoring)
               └─> Context builder (summaries + templates)


2. Public API
Cortex (python)
class Cortex:
    def __init__(self, user_id: str, config: Optional[Config]=None):
        ...

    def ingest(self,
               role: str,               # 'user'|'assistant'|'system'|'event'
               message: str,
               timestamp: Optional[datetime]=None,
               metadata: Optional[dict]=None) -> IngestResult:
        """Single-call API. Library auto-classifies & routes internally."""

    def get_context(self,
                    query: str,
                    max_tokens: int = 1000,
                    structured: bool = False) -> Union[str, ContextStructured]:
        """Returns LLM-ready prompt string or structured dict."""

    # Optional admin / inspection
    def inspect(self, user_id: str, limit: int = 50) -> dict: ...
    def clear(self, user_id: str): ...

Design choices
Minimal public surface. No backend config in typical usage.


Config is optional; advanced users/environment set via env vars.



3. Data model & DB schema (SQLite default via SQLAlchemy)
Tables (simplified)
episodic_messages
id (uuid)


user_id (str)


role (str)


message (text)


tokens (int)


timestamp (datetime)


metadata (json)


prefs
id, user_id, key (str), value (json), confidence (float), source_id (fk), last_updated


facts
id, user_id, text, summary, tags(json), confidence, source_id, created_at


semantic_vectors
id, user_id, text, embedding (binary/float vector), metadata (json), created_at


Indices
user_id on every table, created_at on time-series, full-text index (FTS5) on message/fact text (SQLite).


Pluggable mapping
A Store interface with concrete adapters:


SQLiteStore (default)


PostgresStore (optional; enables pgvector)


InMemoryStore (tests)



4. Ingest pipeline (step-by-step)
Preprocessing


Normalize whitespace, Unicode NFC, remove zero-width chars.


Apply light sanitization (strip tokens, truncate > N chars).


Token count estimation using chosen tokenizer (OpenAI tiktoken or local tokenizer).


Append to episodic buffer


Save raw message with metadata & token count.


Maintain per-user episodic circular buffer (configurable default: last 200 messages or last 5k tokens).


Classification (rule-based fallback + LLM option)


Fast rule engine (primary):


Regex/keyword rules for clear patterns:


Dates/time expressions → candidate fact/event


“I prefer”, “I usually”, “I like” → preference


“My email is”, “My phone” → contact fact (PII gating)


Scheduling language (“book”, “reschedule”, “avoid”) → action / preference


Confidence score (0.6–0.9 depending on heuristics)


LLM classifier (optional / Pro mode)


Small prompt to LLM to label: preference/fact/semantic/ignore/multiple


Use only if rule confidence < threshold or when env allows.


Extraction


Named-entity extraction (dates, locations, organization, email).


Preference normalizer: canonicalize avoid Fridays → {key: "avoid_days", value: ["Friday"]}.


Summarizer candidate: for long user paragraphs, produce short summary (for facts).


Routing


If preference detected → update PreferenceStore (merge with rules).


If fact detected (explicit personal fact / calendar event) → LongTermStore.


If semantic candidate (opinions, long text) → embed & insert into SemanticStore.


Always keep a copy in EpisodicStore (raw).


Dedup & decay


On insert, compute embedding similarity and text similarity to detect duplicates.


Apply TTL / decay rules: preferences persist until overwritten; episodic messages expire by size/time.



5. Semantic storage & embeddings
Defaults
Embeddings: sentence-transformers/all-MiniLM-L6-v2 (local default) OR OpenAI text-embedding-3-small if env set.


Vector index: FAISS (local) backed by disk index files. If DATABASE_URL points to Postgres and pgvector available, use PGVector adapter.


SemanticStore API
class SemanticStore:
    def add(user_id, text, embedding, metadata)
    def query(user_id, embedding, k=5, filter_metadata=None) -> List[Hit]
    def remove(id)

Storage format
Keep embedding vectors in separate file or DB column.


Store metadata with tags: source, timestamp, type (opinion/fact).



6. Retrieval engine — hybrid ranking and context construction
Retrieval steps
Query preprocessing


Tokenize + embed (same model as semantic store).


Extract intent signals (scheduling, preference, info lookup).


Episodic retrieval


Pull last N messages by recency and/or token budget.


Preference & fact filter


Match hard constraints (e.g., avoid days/time) against query intent using simple rule-matchers.


Semantic retrieval


Top-k vector hits (k configurable; default 8).


Optionally re-rank with cross-encoder (Pro).


Merge & rank


Scoring formula (normalized):


score = w_sem * semantic_sim + w_recency * recency_score + w_conf * confidence


w_* configurable; defaults tuned for scheduling & short contexts.


Context building & token budget management
Available tokens = max_tokens input.


Priority:


Hard constraints (preferences) — always include (short).


Facts that match query (high precision).


Most relevant semantic hits (until token budget).


Short excerpt of recent conversation.


Summarization step: if upcoming content + budget > limit, summarize lower-priority chunks with summarizer LLM or extractive heuristics.


Output formats
str prompt block (default).


structured dict:

 {
  "preferences": [...],
  "facts": [...],
  "recent": [...],
  "semantic": [{"text": "...", "score": 0.92}, ...],
  "debug": {"raw_hits": ...}
}



7. Classification: rules, fallback, and extensibility
Rule engine (first line)
Lightweight YAML/JSON rule set with patterns and mapping to memory types.


Example rule:

 - name: avoid_days
  pattern: "\\bavoid(s)? (meetings )?(on )?(?P<day>monday|tuesday|...)\b"
  memory_type: preference
  mapper: days_normalizer
  confidence: 0.9


Run rule set in order; first-match with high confidence will short-circuit most flows.


LLM classifier (optional)
Use when rule confidence < 0.6 or for ambiguous messages.


Prompt template with few-shot examples for high-quality labels.


Cache LLM outputs (idempotent).


Extensibility
Plugin interface ClassifierPlugin to add domain-specific rules/ML models.


Simple registration API: cortex.register_classifier(plugin).



8. Summarization & patching
Short-form summarization: used in context builder and longterm fact condensation. Default uses local extractive summarizer (simple sentence scoring) and optional LLM for abstractive.


Patching/updating facts: when a new statement contradicts existing fact (date change), the library should:


Detect contradiction via entity & date comparison.


Create a new fact entry with active flag; deprecate old one or update with audit.


Keep source_id linking to episodic message to enable provenance.



9. Token & cost optimizations
Always include short preference bullets rather than verbose copies.


Use extractive summarization before sending content to LLM.


Allow get_context(..., summarizer="none"|"local"|"llm").


Cache embeddings & classifier outputs to avoid re-costs.



10. Security, privacy & PII handling
Default: local-only operation. No telemetry.


PII extraction: when messages contain emails, phone numbers, SSNs:


Mask by default in output.


Store as hashed tokens if needed.


Provide opt-in: Cortex(..., pii_mode="store"|"mask"|"ignore").


Encryption for Enterprise: support ENCRYPTION_KEY to encrypt DB files at rest (AES-GCM).


Provide clear(user_id) and export(user_id) for compliance.



11. Config & environment variables (opinionated)
CORTEX_ENV=local|prod


DATABASE_URL=sqlite:///./cortex.db (default)


EMBEDDINGS_PROVIDER=openai|local (default local)


OPENAI_API_KEY (if using OpenAI)


EMBEDDING_MODEL=text-embedding-3-small|all-MiniLM-L6-v2


VECTOR_BACKEND=faiss|pgvector


MAX_EPISODIC_MESSAGES=200


PREFERENCE_CONFIDENCE_THRESHOLD=0.7



12. CLI & dev DX
Commands:
cortex init — create DB, default config


cortex ingest --user u1 --role user --message "..." — test ingest


cortex inspect --user u1 — dump memory


cortex clear --user u1


cortex run — launch local HTTP API (FastAPI) for hosted mode


Local dev mode auto-installs local sentence-transformer weights on first run (with prompt to user).

13. Testing, metrics & evaluation
Tests
Unit: classifier rules, DB adapters, semantic store behavior.


Integration: end-to-end ingest → retrieval pipeline with synthetic conversational traces.


Regression: ensure dedup, TTL, patching logic.


Evaluation metrics
Precision/Recall for preference extraction (manual labeled dataset).


Retrieval relevance: nDCG on need-specific queries (scheduling vs. personal facts).


Token efficiency: average tokens used for context vs. baseline RAG.


Latency: P95 for get_context() under local & hosted modes.



14. Performance & scaling notes
Local dev: FAISS with memory-mapped indices for moderate datasets.


Hosted: use pgvector + Postgres for multi-tenant + reliable persistence.


As dataset grows:


TTL & compaction of episodic messages.


Shard semantic index by user bucket or tenant.


Warm caches for active users.



15. Extension points & plugin API
ClassifierPlugin — custom classification logic.


EmbeddingProvider — register new embedding backends.


VectorBackend — adapters for FAISS / HNSW / PGVector.


Summarizer — replace local summarizer with custom LLM.


StorageAdapter — pluggable DB (DynamoDB, Redis, S3 metadata).



16. Minimal dependency list (opinionated, tiny)
python>=3.10


sqlalchemy


faiss-cpu OR faiss-cpu-wheels (for local)


sentence-transformers


numpy


python-dotenv


fastapi[all] (for hosted option)


uvicorn


pytest (dev)


(Use optional extras: pgvector for postgres mode, openai for OpenAI).

17. Initial sprint (implementation plan: 4 weeks solo/dev)
Week 0 (days 1–7) — core library + ingest
Cortex with ingest() storing episodic messages (SQLite).


Simple rule-based classifier (3–5 rules).


Basic PreferenceStore.


Week 1 (days 8–14) — semantic store & retrieval
Integrate sentence-transformers embeddings + FAISS index.


Implement get_context() to pull preferences + episodic + semantic hits.


Week 2 (days 15–21) — context builder + summarization
Token budgeting & extractive summarizer.


Structured get_context_structured() output.


Week 3 (days 22–28) — CLI, tests, docs
cortex init, inspect, clear


Unit tests and basic integration tests.


README + Quickstart.



18. Success criteria for MVP
ingest() + get_context() work end-to-end locally.


Preference extraction accuracy > 80% on internal test dataset (basic).


1-min cold start index build and <300ms query time for small user (>1k vectors).


Clean, one-page quickstart that devs can copy-paste in <5 minutes.



19. Risks & mitigations (technical)
Brittle classification — mitigation: start rule-based, add optional LLM classifier & feedback loop.


Token waste — mitigation: strict budget, preferences as bullets, extractive summarization.


Scaling vector store — mitigation: shard/tenant indices & suggest PGVector for hosted plan.


PII leakage — mitigation: default mask and opt-in storage, encryption support for enterprise.



20. Deliverables (artifact checklist)
cortex/ package with modules:


cortex.py (public API)


memory/episodic.py, memory/prefs.py, memory/semantic.py, memory/store.py


classification/rules.py


retrieval/engine.py


cli.py


config.py


tests/ with unit & integration tests


examples/quickstart.py


README.md + Quickstart.md




